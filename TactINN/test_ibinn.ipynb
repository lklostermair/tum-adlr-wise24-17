{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FrEIA.FrEIA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibinn_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mibinn_imagenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minvertible_resnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertibleResNet\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibinn_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mibinn_imagenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mheads\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minvertible_multiclass_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertibleMulticlassClassifier\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibinn_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mibinn_imagenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minvertible_imagenet_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertibleImagenetClassifier\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibinn_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mibinn_imagenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CouplingType\n",
      "File \u001b[0;32m~/tum-adlr-wise24-17/ibinn_model/ibinn_imagenet/model/classifiers/invertible_imagenet_classifier.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mFrEIA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFrEIA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mFf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertibleArchitecture\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_16\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_inf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FrEIA.FrEIA'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "from os.path import join\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "\n",
    "# Add the correct path to ibinn_model\n",
    "ibinn_model_path = os.path.join(project_path, 'ibinn_model')\n",
    "sys.path.append(ibinn_model_path)\n",
    "\n",
    "from ibinn_model.ibinn_imagenet.data.imagenet import Imagenet\n",
    "from ibinn_model.ibinn_imagenet.model.backbones.invertible_resnet import InvertibleResNet\n",
    "from ibinn_model.ibinn_imagenet.model.heads.invertible_multiclass_classifier import InvertibleMulticlassClassifier\n",
    "\n",
    "from ibinn_model.ibinn_imagenet.model.classifiers.invertible_imagenet_classifier import InvertibleImagenetClassifier\n",
    "\n",
    "from ibinn_model.ibinn_imagenet.model import CouplingType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option('--checkpoints_out_folder', default='default_out', help='Base directory for all outputs')\n",
    "@click.option('--data_root_folder_train', default='../data/imagenet', help='Root folder for training dataset')\n",
    "@click.option('--data_root_folder_val', default='../data/imagenet', help='Root folder for validation dataset')\n",
    "@click.option('--data_batch_size', default=16, help='Batch size')\n",
    "@click.option('--model_n_loss_dims_1d', default=1024, help='Size of latent space vector')\n",
    "@click.option('--model_coupling_type_name', default='Slow', help='Type of coupling block: GLOW/SLOW')\n",
    "@click.option('--model_clamp', default=0.7, help='Clamp output of coupling block')\n",
    "@click.option('--model_act_norm', default=0.7, help='Scale output of coupling block')\n",
    "@click.option('--model_act_norm_type', default='SOFTPLUS', help='Type of activation normalization of coupling block outputs: SOFTPLUS/SIGMOID/RELU')\n",
    "@click.option('--model_soft_permutation', is_flag=True, help='Enable soft permutation? True/False')\n",
    "@click.option('--model_welling', is_flag=True, help='Enable welling permutation? True/False')\n",
    "@click.option('--model_householder', default=0, help='Use householder permutation? True/False')\n",
    "@click.option('--model_blocks', default='[3,4,6,3]', help='Coupling layers per blocks')\n",
    "@click.option('--model_strides', default='[1,2,2,2]', help='Strides per block type')\n",
    "@click.option('--model_dilations', default='[1,1,1,1]', help='Dilations per block type')\n",
    "@click.option('--model_synchronized_batchnorm', is_flag=True, help='Enable global BatchNorm computation')\n",
    "@click.option('--model_fc_width', default=1024, help='Width of fully-connected layer')\n",
    "@click.option('--training_lr', default=0.07, help='Learning rate')\n",
    "@click.option('--training_lr_mu', default=0.07, help='Learning rate for parameter mu')\n",
    "@click.option('--training_mu_init', default=3.5, help='Initial value for parameter mu')\n",
    "@click.option('--training_mu_conv_init', default=0.01, help='Learning rate for disjoined information')\n",
    "@click.option('--training_mu_low_rank_k', default=128, help='Low-rank approximation')\n",
    "@click.option('--training_n_epochs', default=30, help='Number of training epochs')\n",
    "@click.option('--training_train_nll', is_flag=True, help='Train with negative log likelihood? True/False')\n",
    "@click.option('--training_beta', default='0.5', help='Parameter beta')\n",
    "@click.option('--training_burn_in_iterations', default=10, help='Number of burn in iterations with lower learning rate')\n",
    "@click.option('--checkpoints_interval_log', default=1000, help='Interval to save logs')\n",
    "@click.option('--checkpoints_interval_checkpoint', default=25000, help='Interval to save checkpoints')\n",
    "@click.option('--checkpoints_checkpoint_when_crash', is_flag=True, help='Save checkpoint if training crashes? True/False')\n",
    "@click.option('--checkpoints_resume_checkpoint', is_flag=True, help='Continue from last checkpoint?')\n",
    "@click.option('--checkpoints_cooling_step', default=1, help='Lower learning rate 1e-10 per step')\n",
    "@click.option('--checkpoints_finetune_mu', is_flag=True, help='Only finetune latent space')\n",
    "@click.option('--checkpoints_extension', default='', help='Custom extension to the output model file for ablations')\n",
    "def train(**args):\n",
    "\n",
    "    data = Imagenet(args['data_root_folder_train'], args['data_root_folder_val'], int(args['data_batch_size']))\n",
    "\n",
    "    extension = args['checkpoints_extension']\n",
    "\n",
    "    output_scale = 16\n",
    "    skip_connection = False\n",
    "\n",
    "    n_loss_dims_1d = int(args['model_n_loss_dims_1d'])\n",
    "    n_total_dims_1d = int(3 * data.img_crop_size[0] * data.img_crop_size[1])\n",
    "\n",
    "    coupling_type_name = args['model_coupling_type_name']\n",
    "    coupling_type = CouplingType.GLOW if coupling_type_name == \"GLOW\" else CouplingType.SLOW\n",
    "\n",
    "    finetune_mu = args['checkpoints_finetune_mu']\n",
    "    print(finetune_mu)\n",
    "    backbone = InvertibleResNet(\n",
    "        64,\n",
    "        coupling_type=coupling_type,\n",
    "        block_type=InvertibleResNet.BlockType.BASIC,\n",
    "        clamp=args['model_clamp'],\n",
    "        act_norm=args['model_act_norm'],\n",
    "        act_norm_type=args['model_act_norm_type'],\n",
    "        permute_soft=args['model_soft_permutation'],\n",
    "        welling=args['model_welling'],\n",
    "        householder=int(args['model_householder']),\n",
    "        blocks=eval(args['model_blocks']),\n",
    "        strides=eval(args['model_strides']),\n",
    "        dilations=eval(args['model_dilations']),\n",
    "        synchronized_batchnorm=args['model_synchronized_batchnorm'],\n",
    "        skip_connection=skip_connection\n",
    "    )\n",
    "\n",
    "    head = InvertibleMulticlassClassifier(\n",
    "        int(args['model_fc_width']),\n",
    "        n_loss_dims_1d,\n",
    "        n_total_dims_1d,\n",
    "        coupling_type=coupling_type,\n",
    "        clamp=float(args['model_clamp']),\n",
    "        act_norm=float(args['model_act_norm']),\n",
    "        act_norm_type=args['model_act_norm_type'],\n",
    "        permute_soft=args['model_soft_permutation']\n",
    "    )\n",
    "\n",
    "    inn = InvertibleImagenetClassifier(\n",
    "        float(args['training_lr']) * 10 ** (-float(args['checkpoints_cooling_step'])),\n",
    "        float(args['training_mu_init']),\n",
    "        float(args['training_mu_conv_init']),\n",
    "        args['training_mu_low_rank_k'],\n",
    "        (3, data.img_crop_size[0], data.img_crop_size[1]),\n",
    "        data.n_classes,\n",
    "        n_loss_dims_1d,\n",
    "        n_total_dims_1d,\n",
    "        backbone, head,\n",
    "        finetune_mu = finetune_mu\n",
    "    )\n",
    "    inn.cuda()\n",
    "\n",
    "    inn_parallel = torch.nn.DataParallel(inn)\n",
    "    n_batches_per_epoch = (len(data.train_data.imgs)//data.batch_size)\n",
    "    N_epochs = int(args['training_n_epochs'])\n",
    "\n",
    "    train_nll = args['training_train_nll']\n",
    "    beta = args['training_beta']\n",
    "    if not beta =='infinity':\n",
    "        beta = float(beta)\n",
    "\n",
    "    def truncate(n, decimals=10):\n",
    "        multiplier = 10 ** decimals\n",
    "        return int(n * multiplier) / multiplier\n",
    "\n",
    "    if finetune_mu:\n",
    "        last_lr = float(args['training_lr']) * 10 ** (-(max(0,float(args['checkpoints_cooling_step']))))\n",
    "    else:\n",
    "        last_lr = float(args['training_lr']) * 10 ** (-(max(0,float(args['checkpoints_cooling_step'])-1)))\n",
    "    current_lr = float(args['training_lr']) * 10 ** (-(float(args['checkpoints_cooling_step'])))\n",
    "\n",
    "    last_lr = truncate(last_lr)\n",
    "    current_lr = truncate(current_lr)\n",
    "\n",
    "    if not beta == 'infinity':\n",
    "        if train_nll:\n",
    "            beta_nll = 1. / (1 + beta)\n",
    "            beta_cat_ce = 1. * beta / (1 + beta)\n",
    "        else:\n",
    "            beta_nll, beta_cat_ce = 0., 1.\n",
    "\n",
    "    interval_log = int(args['checkpoints_interval_log'])\n",
    "    # interval_checkpoint = int(args['checkpoints_interval_checkpoint'])\n",
    "\n",
    "    save_on_crash = args['checkpoints_checkpoint_when_crash']\n",
    "    out_folder = args['checkpoints_out_folder']\n",
    "    print('out_folder', out_folder)\n",
    "\n",
    "    finetune_mu_extension = '_finetune_mu' if finetune_mu else \"\"\n",
    "    save_name = '_lr-' + str(current_lr)  + \\\n",
    "    '_nll-' + str(args['training_train_nll']) + \\\n",
    "    '_beta-' + str(args['training_beta']) + \\\n",
    "    '_mbs-' + str(args['data_batch_size']) + \\\n",
    "    '_ct-' + str(args['model_coupling_type_name']) + \\\n",
    "    '_cl-' + str(args['model_clamp']) + \\\n",
    "    '_an-' + str(args['model_act_norm']) + \\\n",
    "    '_blocks-' + str(args['model_blocks']) + \\\n",
    "    'strides-' + str(args['model_strides']) + \\\n",
    "    'dilations-' + str(args['model_dilations']) + \\\n",
    "    '_os-' + str(output_scale) + \\\n",
    "    '_ld-' + str(args['model_fc_width']) + \\\n",
    "    '_k_' + str(args['training_mu_low_rank_k']) + \\\n",
    "    '_ext_' + extension\n",
    "\n",
    "    save_name = save_name + finetune_mu_extension\n",
    "\n",
    "    print(save_name, flush=True)\n",
    "\n",
    "    load_name = '_lr-' + str(last_lr) + \\\n",
    "    '_nll-' + str(args['training_train_nll']) + \\\n",
    "    '_beta-' + str(args['training_beta']) + \\\n",
    "    '_mbs-' + str(args['data_batch_size']) + \\\n",
    "    '_ct-' + str(args['model_coupling_type_name']) + \\\n",
    "    '_cl-' + str(args['model_clamp']) + \\\n",
    "    '_an-' + str(args['model_act_norm']) + \\\n",
    "    '_blocks-' + str(args['model_blocks']) + \\\n",
    "    'strides-' + str(args['model_strides']) + \\\n",
    "    'dilations-' + str(args['model_dilations']) + \\\n",
    "    '_os-' + str(output_scale) + \\\n",
    "    '_ld-' + str(args['model_fc_width']) + \\\n",
    "    '_k_' + str(args['training_mu_low_rank_k']) + \\\n",
    "    '_ext_' + extension\n",
    "\n",
    "    print(load_name, flush=True)\n",
    "\n",
    "    csv = join(out_folder, 'log_train.csv')\n",
    "    csv_val = join(out_folder, 'log_val.csv')\n",
    "\n",
    "    resume = args['checkpoints_resume_checkpoint']\n",
    "\n",
    "    plot_columns = ['time', 'epoch', 'iteration', 'learning_rate',\n",
    "                    'nll_joint_tr', 'nll_class_tr', 'cat_ce_tr', 'acc_tr',\n",
    "                    'nll_joint_val', 'nll_class_val', 'cat_ce_val',\n",
    "                    'acc_val', 'delta_mu_val']\n",
    "\n",
    "    train_loss_names = [l for l in plot_columns if l[-3:] == '_tr']\n",
    "    val_loss_names   = [l for l in plot_columns if l[-4:] == '_val']\n",
    "\n",
    "    header_fmt = '{:>15}' * len(plot_columns)\n",
    "    val_header_fmt = '{:>15}' * len(val_loss_names)\n",
    "    \n",
    "    with open(csv, 'w') as f:\n",
    "        f.write(('{:>15}' * 8).format(*plot_columns) + '\\n')\n",
    "\n",
    "    with open(csv_val, 'w') as f:\n",
    "        f.write(val_header_fmt.format(*val_loss_names) + '\\n')\n",
    "\n",
    "    output_fmt =        '{:15.1f}      {:04d}/{:04d}      {:04d}/{:04d}      {:1.5f}' + '{:15.5f}' * (len(plot_columns) - 4)\n",
    "    live_output_fmt =   '{:15.1f}      {:04d}/{:04d}      {:04d}/{:04d}      {:1.5f}' + '{:15.5f}' * len(train_loss_names) + '\\r'\n",
    "    val_output_fmt =    '{:15.5f}' * len(val_loss_names)\n",
    "\n",
    "    if resume:\n",
    "        if (not beta == \"infinity\") and beta < 0.5:\n",
    "            file_path = join(out_folder, f'{load_name}_best_val_nll.pt')\n",
    "        else:\n",
    "            file_path = join(out_folder, f'{load_name}_best_val_cat_ce.pt')\n",
    "        print(\"Loading \" + file_path, flush=True)\n",
    "        inn.load(file_path)\n",
    "\n",
    "        for param_group in inn.optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "\n",
    "    t_start = time()\n",
    "\n",
    "    best_val_cat_ce = float(\"inf\")\n",
    "    best_val_nll = float(\"inf\")\n",
    "    try:\n",
    "        for i_epoch in range(N_epochs):\n",
    "            print(header_fmt.format(*plot_columns), flush=True)\n",
    "\n",
    "            running_avg = {l: [] for l in train_loss_names}\n",
    "\n",
    "            for i_batch, (x, y) in enumerate(data.train_loader):\n",
    "                x, y = x.cuda().contiguous(), y.cuda().contiguous()\n",
    "\n",
    "                losses = inn_parallel(x, y)\n",
    "                for k, l in losses.items():\n",
    "                    losses[k] = l.mean()\n",
    "                if beta == 'infinity':\n",
    "                    loss = losses['cat_ce_tr']\n",
    "                elif beta == 0.0:\n",
    "                    loss = losses['nll_joint_tr']\n",
    "                else:\n",
    "                    loss = beta_nll * losses['nll_joint_tr'] + beta_cat_ce * losses['cat_ce_tr']\n",
    "\n",
    "                if not resume and i_epoch == 0 and i_batch < int(args['training_burn_in_iterations']):\n",
    "                    loss *= 0.05\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(inn.model_parameters, 8.)\n",
    "                inn.optimizer.step()\n",
    "                inn.optimizer.zero_grad()\n",
    "\n",
    "                losses_display = [(time() - t_start) / 60.,\n",
    "                                  i_epoch, N_epochs,\n",
    "                                  i_batch, len(data.train_loader), inn.lr]\n",
    "\n",
    "                losses_display += [losses[l].item() for l in train_loss_names]\n",
    "                \n",
    "                live_output = live_output_fmt.format(*losses_display)\n",
    "                print(live_output)\n",
    "                with open(csv, 'a') as f:\n",
    "                    f.write(live_output + '\\n')\n",
    "\n",
    "                for l_name in train_loss_names:\n",
    "                    running_avg[l_name].append(losses[l_name].item())\n",
    "\n",
    "                if not ((n_batches_per_epoch * i_epoch) + i_batch) % interval_log:\n",
    "\n",
    "                    for l_name in train_loss_names:\n",
    "                        running_avg[l_name] = np.mean(running_avg[l_name])\n",
    "\n",
    "                    val_avg_losses = {}\n",
    "                    for l_name in val_loss_names:\n",
    "                        val_avg_losses[l_name] = []\n",
    "\n",
    "                    for val_batch, (x, y) in enumerate(data.val_loader_fast):\n",
    "                        with torch.no_grad():\n",
    "                            x = x.cuda()\n",
    "                            y = y.cuda()\n",
    "                            val_losses = inn.validate(x, y)\n",
    "\n",
    "                            for l_name in val_loss_names:\n",
    "                                val_avg_losses[l_name].append(val_losses[l_name].item())\n",
    "\n",
    "                    for l_name in val_loss_names:\n",
    "                        val_avg_losses[l_name] = np.mean(val_avg_losses[l_name])\n",
    "\n",
    "                    for l_name in val_loss_names:\n",
    "                        running_avg[l_name] = val_avg_losses[l_name]\n",
    "\n",
    "                    losses_display = [(time() - t_start) / 60.,\n",
    "                                      i_epoch, N_epochs,\n",
    "                                      i_batch, len(data.train_loader), inn.lr]\n",
    "\n",
    "                    losses_display += [running_avg[l] for l in plot_columns[4:]]\n",
    "                    print(output_fmt.format(*losses_display), flush=True)\n",
    "\n",
    "                    running_avg = {l: [] for l in train_loss_names}\n",
    "\n",
    "            print(val_header_fmt.format(*val_loss_names), flush=True)\n",
    "\n",
    "            val_avg_losses = {}\n",
    "            for l_name in val_loss_names:\n",
    "                val_avg_losses[l_name] = []\n",
    "\n",
    "            for val_batch, (x, y) in enumerate(data.val_loader):\n",
    "                with torch.no_grad():\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                    val_losses = inn.validate(x, y)\n",
    "\n",
    "                    for l_name in val_loss_names:\n",
    "                        val_avg_losses[l_name].append(val_losses[l_name].item())\n",
    "\n",
    "            for l_name in val_loss_names:\n",
    "                val_avg_losses[l_name] = np.mean(val_avg_losses[l_name])\n",
    "\n",
    "            val_losses_display = [val_avg_losses[l] for l in plot_columns[-5:]]\n",
    "\n",
    "            val_output = val_output_fmt.format(*val_losses_display)\n",
    "            print(val_output, flush=True)\n",
    "            with open(csv_val, 'a') as f:\n",
    "                f.write(val_output + '\\n')\n",
    "\n",
    "            if val_avg_losses[\"cat_ce_val\"] < best_val_cat_ce:\n",
    "                inn.save(join(out_folder, f'{save_name}_best_val_cat_ce.pt'))\n",
    "                best_val_cat_ce = val_avg_losses[\"cat_ce_val\"]\n",
    "            if val_avg_losses[\"nll_joint_val\"] < best_val_nll:\n",
    "                inn.save(join(out_folder, f'{save_name}_best_val_nll.pt'))\n",
    "                best_val_nll = val_avg_losses[\"nll_joint_val\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e, flush=True)\n",
    "        if save_on_crash:\n",
    "            inn.save(join(out_folder, f'{save_name}_ABORT.pt'))\n",
    "        raise\n",
    "    \n",
    "    print('out_folder', out_folder)\n",
    "    inn.save(join(out_folder, f'{save_name}.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(train())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
