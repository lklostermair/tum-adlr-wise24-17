{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import FrEIA.framework as Ff\n",
    "import FrEIA.modules as Fm\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import TactileMaterialDataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class SimpleCNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    A small 1D CNN that processes [B, 16, T] and outputs scale + shift for an (16, T)-shaped input.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=16,       # 16 taxel channels\n",
    "                 hidden_channels=16,\n",
    "                 kernel_size=3,\n",
    "                 length=1000,         # Temporal length\n",
    "                 label_dim=0):\n",
    "        super(SimpleCNN1D, self).__init__()\n",
    "        self.label_dim = label_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.length = length\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.in_channels + (1 if label_dim > 0 else 0),\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2, stride=2)  # Halves the temporal length\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Flattened dimension after pooling\n",
    "        reduced_length = self.length // 2\n",
    "        self.flatten_dim = hidden_channels * reduced_length\n",
    "\n",
    "        # Output scale and shift\n",
    "        self.fc_out = nn.Linear(self.flatten_dim, 2 * in_channels * self.length)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"\n",
    "        x: [B, 16, T] - Input tensor\n",
    "        y: [B, label_dim] - Optional label embedding\n",
    "        \"\"\"\n",
    "        if y is not None:\n",
    "            # Broadcast label embedding to match input temporal length\n",
    "            y = y.unsqueeze(-1).expand(-1, -1, x.size(-1))  # [B, label_dim, T]\n",
    "            x = torch.cat([x, y], dim=1)  # [B, in_channels + label_dim, T]\n",
    "\n",
    "        x = self.conv1(x)  # [B, hidden_channels, T]\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)  # [B, hidden_channels, T]\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # [B, hidden_channels, T/2]\n",
    "\n",
    "        # Flatten and compute scale + shift\n",
    "        x = x.flatten(1)  # [B, hidden_channels * (T/2)]\n",
    "        x = self.fc_out(x)  # [B, 2 * (16 * T)]\n",
    "        scale, shift = torch.chunk(x, 2, dim=1)  # Each => [B, 16 * T]\n",
    "        return scale, shift\n",
    "\n",
    "\n",
    "\n",
    "def affine_coupling_block(in_channels, hidden_dim=64):\n",
    "    def subnet_constructor(c_in, c_out):\n",
    "        return SimpleCNN1D(in_channels, hidden_dim, label_dim=0)\n",
    "\n",
    "    return Fm.AllInOneBlock(\n",
    "        dims_in=[(in_channels,)],\n",
    "        subnet_constructor=subnet_constructor,\n",
    "        affine_clamping=2.0\n",
    "    )\n",
    "\n",
    "class InvertibleFlow(nn.Module):\n",
    "    def __init__(self, input_dim, n_blocks=4, hidden_dim=64):\n",
    "        super(InvertibleFlow, self).__init__()\n",
    "        self.flow = Ff.SequenceINN(input_dim)\n",
    "        for _ in range(n_blocks):\n",
    "            self.flow.append(Fm.PermuteRandom(dims_in=[(input_dim,)], seed=0))\n",
    "            self.flow.append(affine_coupling_block(input_dim, hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, log_detJ = self.flow(x)\n",
    "        return z, log_detJ\n",
    "\n",
    "    def inverse(self, z):\n",
    "        x, log_detJ = self.flow(z, rev=True)\n",
    "        return x, log_detJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionalGMM(nn.Module):\n",
    "    def __init__(self, n_classes, latent_dim):\n",
    "        super(ClassConditionalGMM, self).__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(n_classes, latent_dim))\n",
    "        self.logvar = nn.Parameter(torch.zeros(n_classes, latent_dim))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Compute log-probabilities for each class\n",
    "        z_expanded = z.unsqueeze(1)\n",
    "        mu = self.mu.unsqueeze(0)\n",
    "        logvar = self.logvar.unsqueeze(0)\n",
    "        log_p = -0.5 * (logvar + (z_expanded - mu)**2 / logvar.exp())\n",
    "        return log_p.sum(dim=-1)\n",
    "\n",
    "def combined_loss(x, c, model, gmm, beta=1.0):\n",
    "    z, log_detJ = model(x)\n",
    "    log_pz = -0.5 * (z**2).sum(dim=1)\n",
    "    L_gen = -(log_pz + log_detJ).mean()\n",
    "\n",
    "    log_pc = gmm(z)\n",
    "    L_cls = nn.CrossEntropyLoss()(log_pc, c)\n",
    "    return L_gen + beta * L_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(h5_file, epochs=10, batch_size=32, lr=1e-3):\n",
    "    # Dataset & DataLoader\n",
    "    train_set = TactileMaterialDataset(h5_file, split='train')\n",
    "    val_set = TactileMaterialDataset(h5_file, split='val')\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model and Optimizer\n",
    "    input_dim = 16 * 1000\n",
    "    model = InvertibleFlow(input_dim, n_blocks=4, hidden_dim=64)\n",
    "    gmm = ClassConditionalGMM(n_classes=36, latent_dim=input_dim)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(gmm.parameters()), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        gmm.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=\"Training\", leave=False) as pbar:\n",
    "            for x_batch, c_batch in train_loader:\n",
    "                x_batch = x_batch.view(x_batch.size(0), -1)  # Flatten [B, T, C] -> [B, T*C]\n",
    "                optimizer.zero_grad()\n",
    "                loss = combined_loss(x_batch, c_batch, model, gmm)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        gmm.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation\", leave=False) as pbar:\n",
    "            for x_batch, c_batch in val_loader:\n",
    "                x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "                with torch.no_grad():\n",
    "                    loss = combined_loss(x_batch, c_batch, model, gmm)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    z, _ = model(x_batch)\n",
    "                    preds = gmm(z).argmax(dim=1)\n",
    "                    correct += (preds == c_batch).sum().item()\n",
    "                    total += c_batch.size(0)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2880\n",
      "Val dataset size: 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luki/myenv/lib/python3.12/site-packages/FrEIA/modules/all_in_one_block.py:54: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 4096000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/luki/tum-adlr-wise24-17/data/raw/tactmat.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(h5_file, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Model and Optimizer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mInvertibleFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m gmm \u001b[38;5;241m=\u001b[39m ClassConditionalGMM(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(gmm\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mInvertibleFlow.__init__\u001b[0;34m(self, input_dim, n_blocks, hidden_dim)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_blocks):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mappend(Fm\u001b[38;5;241m.\u001b[39mPermuteRandom(dims_in\u001b[38;5;241m=\u001b[39m[(input_dim,)], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mappend(\u001b[43maffine_coupling_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m, in \u001b[0;36maffine_coupling_block\u001b[0;34m(in_channels, hidden_dim)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubnet_constructor\u001b[39m(c_in, c_out):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SimpleCNN1D(in_channels, hidden_dim, label_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAllInOneBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdims_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubnet_constructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubnet_constructor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43maffine_clamping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/FrEIA/modules/all_in_one_block.py:171\u001b[0m, in \u001b[0;36mAllInOneBlock.__init__\u001b[0;34m(self, dims_in, dims_c, subnet_constructor, affine_clamping, gin_block, global_affine_init, global_affine_type, permute_soft, learned_householder_permutation, reverse_permutation)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subnet_constructor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease supply a callable subnet_constructor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction or object (see docstring)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubnet \u001b[38;5;241m=\u001b[39m \u001b[43msubnet_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m, in \u001b[0;36maffine_coupling_block.<locals>.subnet_constructor\u001b[0;34m(c_in, c_out)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubnet_constructor\u001b[39m(c_in, c_out):\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimpleCNN1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mSimpleCNN1D.__init__\u001b[0;34m(self, in_channels, hidden_channels, kernel_size, length, label_dim)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_dim \u001b[38;5;241m=\u001b[39m hidden_channels \u001b[38;5;241m*\u001b[39m reduced_length\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Output scale and shift\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/linear.py:106\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 4096000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model(h5_file=\"/home/luki/tum-adlr-wise24-17/data/raw/tactmat.h5\", epochs=10, batch_size=16, lr=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
